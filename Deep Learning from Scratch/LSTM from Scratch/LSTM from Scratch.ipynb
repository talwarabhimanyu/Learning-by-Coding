{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This notebook builds an LSTM model without using any Deep Learning libraries. It accompanies my [blog post](https://talwarabhimanyu.github.io/blog/2018/08/12/lstm-backprop) which explains the mathematics behind forward-pass and more importantly backpropogation involved with training an LSTM.\n",
    "\n",
    "To organize various Python functions involved in training, I have used the same layout as in [Assignment 3 of Stanford's CS231n course](http://cs231n.github.io/assignments2016/assignment3/) - the code for forward-pass and backpropogation is my own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure: A Single Time-Step in an LSTM\n",
    "<img src=\"../../images/LSTM Diagram.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def tanh(x):\n",
    "    expo = np.exp(-2*np.clip(x, -500, 500))\n",
    "    return (1 - expo)/(1 + expo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation for LSTM Units\n",
    "An LSTM Unit at time-step $t$ takes as input: <br/>\n",
    "* a minibatch of 'words' denoted by $x^{(t)}$, of dimensions $N \\times d$, and <br/>\n",
    "* the 'hidden-state' vector $h^{(t-1)}$ from the previous unit, of dimensions $N \\times D$, and </br>\n",
    "* the 'internal-state' vector $s^{(t-1)}$ from the previous unit, of dimensions $N \\times D$.\n",
    "\n",
    "**Note: $d$ and $D$ are hyper-parmaters, i.e. we _chose_ to represent each hidden/internal state using a vector of length $D$ and we _chose_ to use 'word embedding' vectors of length $d$.**\n",
    "\n",
    "The code below implements a single LSTM Unit's computation. The output is two vectors - the 'hidden-state' $h^{(t)}$, and the 'internal-state' $s^{(t)}$ - each of dimensions $N \\times D_h$. (In this notebook, $h^{(t)}$ for time-step $t$ is always referred to as $h\\_next$. Similar notation is used for $s^{(t)}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_step_forward(x_t, h_prev, s_prev, params):\n",
    "    # Load parameters\n",
    "    We = params['We']\n",
    "    Wf = params['Wf']\n",
    "    Wg = params['Wg']\n",
    "    Wq = params['Wq']\n",
    "    Ue = params['Ue']\n",
    "    Uf = params['Uf']\n",
    "    Ug = params['Ug']\n",
    "    Uq = params['Uq']\n",
    "    be = params['be']\n",
    "    bf = params['bf']\n",
    "    bg = params['bg']\n",
    "    bq = params['bq']\n",
    "    # Compute gate values\n",
    "    e_t = sigmoid(be + np.matmul(x_t, Ue.T) + np.matmul(h_prev, We.T))\n",
    "    f_t = sigmoid(bf + np.matmul(x_t, Uf.T) + np.matmul(h_prev, Wf.T))\n",
    "    g_t = sigmoid(bg + np.matmul(x_t, Ug.T) + np.matmul(h_prev, Wg.T))\n",
    "    q_t = sigmoid(bq + np.matmul(x_t, Uq.T) + np.matmul(h_prev, Wq.T))\n",
    "    # Compute signals\n",
    "    s_next = f_t*s_prev + g_t*e_t\n",
    "    h_next = q_t*tanh(s_next)\n",
    "    cache = {'s_prev' : s_prev, \n",
    "             's_next' : s_next, \n",
    "             'x_t' : x_t, \n",
    "             'e_t' : e_t, \n",
    "             'f_t' : f_t, \n",
    "             'g_t' : g_t, \n",
    "             'q_t' : q_t, \n",
    "             'h_prev' : h_prev}\n",
    "    return h_next, s_next, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass through All Time-Steps\n",
    "An LSTM Unit depends on the previous LSTM Unit's hidden/internal states (this is not different from any plain feedforward network). Therefore we sequentially run the $lstm\\_step\\_forward$ method implemented above, for each time step.\n",
    "\n",
    "**Note: One crucial difference from a plain feedforward network is that each LSTM Unit uses the same parameters ($W_f$, $U_f$, $b_f$ etc.). This point of difference will have a significant bearing on how we backprop through an LSTM.**\n",
    "\n",
    "The code below implements the forward pass through an LSTM. We are given as inputs:\n",
    "* a minibatch of 'word sequences' denoted by $x$, of dimensions $N \\times T \\times d$, where $N$ is the numnber of minibatches and $T$ is the length of each sequence,\n",
    "* initial hidden/internal state vectors denoted by $h^{(0)}$ and $s^{(0)}$ of dimensions $N \\times D$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(T, x, h_0, s_0, params):\n",
    "    N, T, d = x.shape\n",
    "    _, D = h_0.shape\n",
    "    h = np.zeros((N, T, D))\n",
    "    h_prev = h_0\n",
    "    s_prev = s_0\n",
    "    cache_dict = {}\n",
    "    for t in range(T):\n",
    "        h[:, t, :], s_next, cache_step = lstm_step_forward(x[:,t,:], h_prev, s_prev, params) \n",
    "        h_prev = h[:, t, :]\n",
    "        s_prev = s_next\n",
    "        cache_dict.update({t : cache_step})\n",
    "    return h, cache_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropogation\n",
    "At each time-step $t$, we are given as inputs:\n",
    "* the cache for this time-step saved during our forward pass - cache stores a bunch of quantities which were computed during the forward pass,\n",
    "* the gradients of total loss $J$ with respect to $h^{(t)}$ and $s^{(t)}$, denoted by $dh\\_next$ and $ds\\_next$ respectively, each of dimensions $N \\times D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_step_backward(dh_next, ds_next, cache, params):\n",
    "    \"\"\"\n",
    "    dh_next is of shape (N, D)\n",
    "    ds_next is of shape (N, D)\n",
    "    \"\"\"\n",
    "    # Load parameters\n",
    "    We = params['We']\n",
    "    Wf = params['Wf']\n",
    "    Wg = params['Wg']\n",
    "    Wq = params['Wq']\n",
    "    Ue = params['Ue']\n",
    "    Uf = params['Uf']\n",
    "    Ug = params['Ug']\n",
    "    Uq = params['Uq']\n",
    "    be = params['be']\n",
    "    bf = params['bf']\n",
    "    bg = params['bg']\n",
    "    bq = params['bq']\n",
    "    # Load cached quantities\n",
    "    s_prev = cache['s_prev']\n",
    "    s_next = cache['s_next']\n",
    "    x_t = cache['x_t']\n",
    "    e_t = cache['e_t']\n",
    "    f_t = cache['f_t']\n",
    "    g_t = cache['g_t']\n",
    "    q_t = cache['q_t']\n",
    "    h_prev = cache['h_prev']\n",
    "    \n",
    "    # Compute frequently used quantities\n",
    "    tanh_s = tanh(s_next)\n",
    "    \n",
    "    # Internal state s\n",
    "    ds_next = dh_next*q_t*(1-tanh_s**2) + ds_next\n",
    "    \n",
    "    # Forget gate f\n",
    "    df_step = ds_next*s_prev\n",
    "    dsigmoid_f = f_t*(1 - f_t)\n",
    "    f_temp = df_step*dsigmoid_f\n",
    "    dUf_step = np.matmul(f_temp.T, x_t) \n",
    "    dWf_step = np.matmul(f_temp.T, h_prev)\n",
    "    dbf_step = np.sum(f_temp, axis=0)\n",
    "    \n",
    "    # Input gate g\n",
    "    dg_step = ds_next*e_t\n",
    "    dsigmoid_g = g_t*(1 - g_t)\n",
    "    g_temp = dg_step*dsigmoid_g\n",
    "    dUg_step = np.matmul(g_temp.T, x_t) \n",
    "    dWg_step = np.matmul(g_temp.T, h_prev)\n",
    "    dbg_step = np.sum(g_temp, axis=0)\n",
    "    \n",
    "    # Output gate q\n",
    "    dq_step = dh_next*tanh_s\n",
    "    dsigmoid_q = q_t*(1 - q_t)\n",
    "    q_temp = dq_step*dsigmoid_q\n",
    "    dUq_step = np.matmul(q_temp.T, x_t) \n",
    "    dWq_step = np.matmul(q_temp.T, h_prev)\n",
    "    dbq_step = np.sum(q_temp, axis=0)\n",
    "    \n",
    "    # Input transform e\n",
    "    de_step = ds_next*g_t\n",
    "    dsigmoid_e = e_t*(1 - e_t)\n",
    "    e_temp = de_step*dsigmoid_e\n",
    "    dUe_step = np.matmul(e_temp.T, x_t) \n",
    "    dWe_step = np.matmul(e_temp.T, h_prev)\n",
    "    dbe_step = np.sum(e_temp, axis=0)\n",
    "    \n",
    "    # Gradient w.r.t previous state h_prev\n",
    "    dh_prev = np.matmul(dh_next*tanh_s*dsigmoid_q, Wq) \\\n",
    "                    + np.matmul(ds_next*s_prev*dsigmoid_f, Wf) \\\n",
    "                    + np.matmul(ds_next*g_t*dsigmoid_e, We) \\\n",
    "                    + np.matmul(ds_next*e_t*dsigmoid_g, Wg)           \n",
    "    ds_prev = f_t*ds_next\n",
    "    grads = {'We' : dWe_step, 'Wf' : dWf_step, 'Wg' : dWg_step, 'Wq' : dWq_step,\n",
    "              'Ue' : dUe_step, 'Uf' : dUf_step, 'Ug' : dUg_step, 'Uq' : dUq_step,\n",
    "              'be' : dbe_step, 'bf' : dbf_step, 'bg' : dbg_step, 'bq' : dbq_step\n",
    "            }\n",
    "    return dh_prev, ds_prev, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_backward(dh, cache_dict, params):\n",
    "    N, T, D = dh.shape\n",
    "    _, d = params['Ue'].shape\n",
    "    all_grads = {key: np.zeros_like(params[key]) for key in params} \n",
    "    dh_next = np.zeros((N, D))\n",
    "    ds_next = np.zeros((N, D))\n",
    "    for t in range(T, 0, -1):\n",
    "        dh_next += dh[:, t-1, :]\n",
    "        dh_prev, ds_prev, step_grads = lstm_step_backward(dh_next, ds_next, cache_dict[t-1], params)\n",
    "        dh_next = dh_prev\n",
    "        ds_next = ds_prev\n",
    "        # Accumulate gradients\n",
    "        for key in step_grads:\n",
    "            all_grads[key] += step_grads[key]\n",
    "    return all_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation for the Affine Layer \n",
    "Atop each LSTM Unit, sits an Affine layer which takes the vector $h^{(t)}$ as input, applies an Affine transformation, and computes the Softmax Probability. The parameters of this layer are $U \\space (Dim: D \\times V )$ and $b_2 \\space (Dim: V \\times 1)$. \n",
    "\n",
    "We do not have to implement a separate Affine layer for each time-step. Unlike in the case of computation inside an LSTM Unit, where the computation depended on output of the previous LSTM unit, the Affine computations at each time-step are independent of each other (i.e. once $h^{(t)}$ has been computed for all time-steps). Therefore, we will perform the Affine computation for ALL $T$ time-steps in one go, taking into impact the contribution from ALL mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(h, U, b2):\n",
    "    N, T, Dh = h.shape\n",
    "    V = b2.shape[0]\n",
    "    theta = (np.matmul(h.reshape(N*T, Dh), U.T) + b2).reshape(N, T, V)\n",
    "    cache = U, b2, h\n",
    "    return theta, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_backward(dtheta, cache):\n",
    "    U, b2, h = cache\n",
    "    Dh = U.shape[1]\n",
    "    N, T, V = dtheta.shape\n",
    "    dh = np.matmul(dtheta.reshape(N*T, V), U).reshape(N, T, Dh)\n",
    "    dU = np.matmul((dtheta.reshape(N*T, V).T), h.reshape(N*T, Dh))\n",
    "    db2 = dtheta.sum(axis=(0,1))\n",
    "    return dh, dU, db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation for the Softmax Layer\n",
    "We will compute probabilities for all $T$ sequences in a minibatch, over all $N$ minibatches, in one go. This follows from similar reasoning as I described for the Affine layer above.\n",
    "\n",
    "**Inputs:**\n",
    "* Matrix $\\theta$ of dimensions $N \\times T \\times V$ which stores the output of Affine Layers, and\n",
    "* Matrix $y$ of dimensions $N \\times T$ which stores the index in Vocabulary of the true 'word' for each time-step, for each minibatch.\n",
    "\n",
    "**Outputs:**\n",
    "* Loss over all minibatches (a single floating point number), and\n",
    "* Matrix $dtheta$ of same dimensions as $\\theta$, and which stores gradients of Loss w.r.t $\\theta$.\n",
    "\n",
    "**Notes:** \n",
    "* I have directly lifted the $softmax\\_loss$ function from the starter code of Assignment 3 of Stanford's CS231n's [Winter 2016 edition](http://cs231n.stanford.edu/2016/).\n",
    "* This version of $softmax\\_loss$ uses a 'mask', an array of dimensions $N \\times T$ which indicates which time-steps in a minibatch should not be counted towards the loss. This is used to handle sequences whose length is less than $T$ - we pad them with zeros (in my implementation) to increase their length to $T$, which makes for easy code elsewhere. \n",
    "* I have previously discussed maths for backprop through a Softmax layer in a [blog post](https://talwarabhimanyu.github.io/blog/2017/05/20/softmax-backprop). You may refer to $Equation \\space 1.3$ in that post which derives the gradient of Loss w.r.t $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(theta, y, mask):\n",
    "    N, T, V = theta.shape\n",
    "    theta_flat = theta.reshape(N*T, V)\n",
    "    y_flat = y.reshape(N*T)\n",
    "    mask_flat = mask.reshape(N*T)\n",
    "    \n",
    "    probs = np.exp(theta_flat - np.max(theta_flat, axis=1, keepdims=True))\n",
    "    probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "    loss = -np.sum(mask_flat * np.log(probs[np.arange(N * T), y_flat])) / N\n",
    "    dtheta_flat = probs.copy()\n",
    "    dtheta_flat[np.arange(N * T), y_flat] -= 1\n",
    "    dtheta_flat /= N\n",
    "    dtheta_flat *= mask_flat[:, None]\n",
    "    \n",
    "    dtheta = dtheta_flat.reshape(N, T, V)\n",
    "    return loss, dtheta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Evaluation of Gradients to Check Correctness of our Implementation\n",
    "If you are familiar with how to numerically check gradients for a network, you can skip this section and move on to [Training our LSTM](#sec_id)\n",
    "\n",
    "Below, the function $eval\\_grad$ evaluates the gradient of a given function $f$ at a point $x$. This point $x$ can be multidimensional, for example I will use the $2D$ matrix $W_h$ as a 'point'. The 'gradient' is basically the change in Loss due to an infinitesimally small perturbation to the point $x$.\n",
    "\n",
    "Notice in the code below that I have multiplied by $df$ to calculate the gradient. This is because we are going to be passing $lstm\\_forward$ and $lstm\\_step\\_forward$ for the argument $f$. Both these functions return the vector $h$ and not the scalar Loss which we need to compute the gradient w.r.t point $x$. Therefore we need to multiple by $dh$ to get our gradient, which is what we pass for the argument $df$.\n",
    "\n",
    "**Note: Below, I have used the numerical gradient evaluation functions provided for assignments of Stanford's course CS321n, _\"Convolutional Neural Networks for Visual Recognition\"_. Specifically, I have used code from the [Winter 2016 edition](http://cs231n.stanford.edu/2016/).  **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_grad(f, x, df):\n",
    "    grad = np.zeros_like(x)\n",
    "    epsilon = 1e-5\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        orig_val = x[idx]\n",
    "        x[idx] = orig_val + epsilon\n",
    "        fwd_fx = f(x)\n",
    "        x[idx] = orig_val - epsilon\n",
    "        bck_fx = f(x)\n",
    "        grad[idx] = np.sum((fwd_fx - bck_fx)*df/(epsilon*2))\n",
    "        x[idx] = orig_val\n",
    "        it.iternext()\n",
    "    return grad\n",
    "def rel_error(x, y):\n",
    "    # Returns relative error\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in dWe: 2.859711404201946e-10\n",
      "Error in dWf: 2.7817214490791844e-09\n",
      "Error in dWg: 8.596699600007139e-10\n",
      "Error in dWq: 1.7290091323325574e-09\n",
      "\n",
      "\n",
      "Error in dUe: 3.807242366831024e-09\n",
      "Error in dUf: 3.3221298997976607e-08\n",
      "Error in dUg: 1.6343065069759545e-10\n",
      "Error in dUq: 4.376333502803696e-10\n",
      "\n",
      "\n",
      "Error in dbe: 6.885219031776487e-11\n",
      "Error in dbf: 6.716352031190419e-10\n",
      "Error in dbg: 2.949556129459655e-10\n",
      "Error in dbq: 1.848974550920749e-11\n",
      "\n",
      "\n",
      "Error in dh_prev: 1.940746476877571e-09\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Check Gradients for a single LSTM Unit \"\"\"\n",
    "\n",
    "np.random.seed(10151)\n",
    "N, D, d = 10, 5, 3\n",
    "# Parameters\n",
    "We = np.random.randn(D, D)\n",
    "Wf = np.random.randn(D, D)\n",
    "Wg = np.random.randn(D, D)\n",
    "Wq = np.random.randn(D, D)\n",
    "\n",
    "be = np.random.randn(D)\n",
    "bf = np.random.randn(D)\n",
    "bg = np.random.randn(D)\n",
    "bq = np.random.randn(D)\n",
    "\n",
    "Ue = np.random.randn(D, d)\n",
    "Uf = np.random.randn(D, d)\n",
    "Ug = np.random.randn(D, d)\n",
    "Uq = np.random.randn(D, d)\n",
    "\n",
    "params = {'We' : We, 'Wf' : Wf, 'Wg' : Wg, 'Wq' : Wq,\n",
    "          'Ue' : Ue, 'Uf' : Uf, 'Ug' : Ug, 'Uq' : Uq,\n",
    "          'be' : be, 'bf' : bf, 'bg' : bg, 'bq' : bq\n",
    "         }\n",
    "\n",
    "# Inputs\n",
    "x_t = np.random.randn(N, d)\n",
    "h_prev = np.random.randn(N, D)\n",
    "s_prev = np.random.randn(N, D)\n",
    "\n",
    "# Test functions\n",
    "fWe_h = lambda We: lstm_step_forward(x_t, h_prev, s_prev, params)[0]\n",
    "fWf_h = lambda Wf: lstm_step_forward(x_t, h_prev, s_prev, params)[0]\n",
    "fWg_h = lambda Wg: lstm_step_forward(x_t, h_prev, s_prev, params)[0]\n",
    "fWq_h = lambda Wq: lstm_step_forward(x_t, h_prev, s_prev, params)[0]\n",
    "\n",
    "fUe_h = lambda Ue: lstm_step_forward(x_t, h_prev, s_prev, params)[0]\n",
    "fUf_h = lambda Uf: lstm_step_forward(x_t, h_prev, s_prev, params)[0]\n",
    "fUg_h = lambda Ug: lstm_step_forward(x_t, h_prev, s_prev, params)[0]\n",
    "fUq_h = lambda Uq: lstm_step_forward(x_t, h_prev, s_prev, params)[0]\n",
    "\n",
    "fbe_h = lambda be: lstm_step_forward(x_t, h_prev, s_prev, params)[0]\n",
    "fbf_h = lambda bf: lstm_step_forward(x_t, h_prev, s_prev, params)[0]\n",
    "fbg_h = lambda bg: lstm_step_forward(x_t, h_prev, s_prev, params)[0]\n",
    "fbq_h = lambda bq: lstm_step_forward(x_t, h_prev, s_prev, params)[0]\n",
    "\n",
    "fWe_s = lambda We: lstm_step_forward(x_t, h_prev, s_prev, params)[1]\n",
    "fWf_s = lambda Wf: lstm_step_forward(x_t, h_prev, s_prev, params)[1]\n",
    "fWg_s = lambda Wg: lstm_step_forward(x_t, h_prev, s_prev, params)[1]\n",
    "fWq_s = lambda Wq: lstm_step_forward(x_t, h_prev, s_prev, params)[1]\n",
    "\n",
    "fUe_s = lambda Ue: lstm_step_forward(x_t, h_prev, s_prev, params)[1]\n",
    "fUf_s = lambda Uf: lstm_step_forward(x_t, h_prev, s_prev, params)[1]\n",
    "fUg_s = lambda Ug: lstm_step_forward(x_t, h_prev, s_prev, params)[1]\n",
    "fUq_s = lambda Uq: lstm_step_forward(x_t, h_prev, s_prev, params)[1]\n",
    "\n",
    "fbe_s = lambda be: lstm_step_forward(x_t, h_prev, s_prev, params)[1]\n",
    "fbf_s = lambda bf: lstm_step_forward(x_t, h_prev, s_prev, params)[1]\n",
    "fbg_s = lambda bg: lstm_step_forward(x_t, h_prev, s_prev, params)[1]\n",
    "fbq_s = lambda bq: lstm_step_forward(x_t, h_prev, s_prev, params)[1]\n",
    "\n",
    "fh_prev_h = lambda h_prev: lstm_step_forward(x_t, h_prev, s_prev, params)[0]\n",
    "fh_prev_s = lambda h_prev: lstm_step_forward(x_t, h_prev, s_prev, params)[1]\n",
    "\n",
    "# Evaluate test functions\n",
    "h_next, s_next, cache_step = lstm_step_forward(x_t, h_prev, s_prev, params)\n",
    "dh_next = np.random.randn(*h_next.shape)\n",
    "ds_next = np.random.randn(*s_next.shape)\n",
    "dh_prev, ds_prev, grads = lstm_step_backward(dh_next, ds_next, cache_step, params)\n",
    "\n",
    "dWe_num = eval_grad(fWe_h, We, dh_next) + eval_grad(fWe_s, We, ds_next)\n",
    "dWf_num = eval_grad(fWf_h, Wf, dh_next) + eval_grad(fWf_s, Wf, ds_next)\n",
    "dWg_num = eval_grad(fWg_h, Wg, dh_next) + eval_grad(fWg_s, Wg, ds_next)\n",
    "dWq_num = eval_grad(fWq_h, Wq, dh_next) + eval_grad(fWq_s, Wq, ds_next)\n",
    "\n",
    "dUe_num = eval_grad(fUe_h, Ue, dh_next) + eval_grad(fUe_s, Ue, ds_next)\n",
    "dUf_num = eval_grad(fUf_h, Uf, dh_next) + eval_grad(fUf_s, Uf, ds_next)\n",
    "dUg_num = eval_grad(fUg_h, Ug, dh_next) + eval_grad(fUg_s, Ug, ds_next)\n",
    "dUq_num = eval_grad(fUq_h, Uq, dh_next) + eval_grad(fUq_s, Uq, ds_next)\n",
    "\n",
    "dbe_num = eval_grad(fbe_h, be, dh_next) + eval_grad(fbe_s, be, ds_next)\n",
    "dbf_num = eval_grad(fbf_h, bf, dh_next) + eval_grad(fbf_s, bf, ds_next)\n",
    "dbg_num = eval_grad(fbg_h, bg, dh_next) + eval_grad(fbg_s, bg, ds_next)\n",
    "dbq_num = eval_grad(fbq_h, bq, dh_next) + eval_grad(fbq_s, bq, ds_next)\n",
    "\n",
    "dh_prev_num = eval_grad(fh_prev_h, h_prev, dh_next) + eval_grad(fh_prev_s, h_prev, ds_next)\n",
    "\n",
    "print('Error in dWe: {}'.format(rel_error(dWe_num, grads['We'])))\n",
    "print('Error in dWf: {}'.format(rel_error(dWf_num, grads['Wf'])))\n",
    "print('Error in dWg: {}'.format(rel_error(dWg_num, grads['Wg'])))\n",
    "print('Error in dWq: {}'.format(rel_error(dWq_num, grads['Wq'])))\n",
    "print('\\n')\n",
    "print('Error in dUe: {}'.format(rel_error(dUe_num, grads['Ue'])))\n",
    "print('Error in dUf: {}'.format(rel_error(dUf_num, grads['Uf'])))\n",
    "print('Error in dUg: {}'.format(rel_error(dUg_num, grads['Ug'])))\n",
    "print('Error in dUq: {}'.format(rel_error(dUq_num, grads['Uq'])))\n",
    "print('\\n')\n",
    "print('Error in dbe: {}'.format(rel_error(dbe_num, grads['be'])))\n",
    "print('Error in dbf: {}'.format(rel_error(dbf_num, grads['bf'])))\n",
    "print('Error in dbg: {}'.format(rel_error(dbg_num, grads['bg'])))\n",
    "print('Error in dbq: {}'.format(rel_error(dbq_num, grads['bq'])))\n",
    "print('\\n')\n",
    "print('Error in dh_prev: {}'.format(rel_error(dh_prev_num, dh_prev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in dWe: 1.301420817525277e-08\n",
      "Error in dWf: 7.115199565178917e-10\n",
      "Error in dWg: 9.268257581061839e-09\n",
      "Error in dWq: 1.2087991093064742e-09\n",
      "\n",
      "\n",
      "Error in dUe: 7.708884312846093e-09\n",
      "Error in dUf: 1.0165199498294216e-09\n",
      "Error in dUg: 5.196960122411291e-08\n",
      "Error in dUq: 2.4545406556922528e-09\n",
      "\n",
      "\n",
      "Error in dbe: 7.696329593761408e-10\n",
      "Error in dbf: 4.398797000208122e-10\n",
      "Error in dbg: 1.2224354996155846e-10\n",
      "Error in dbq: 1.4806447099098493e-10\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Check Gradients for the entire LSTM \"\"\"\n",
    "\n",
    "np.random.seed(10151)\n",
    "T, N, D, d = 5, 7, 5, 10\n",
    "# Parameters\n",
    "We = np.random.randn(D, D)\n",
    "Wf = np.random.randn(D, D)\n",
    "Wg = np.random.randn(D, D)\n",
    "Wq = np.random.randn(D, D)\n",
    "\n",
    "Ue = np.random.randn(D, d)\n",
    "Uf = np.random.randn(D, d)\n",
    "Ug = np.random.randn(D, d)\n",
    "Uq = np.random.randn(D, d)\n",
    "\n",
    "be = np.random.randn(D)\n",
    "bf = np.random.randn(D)\n",
    "bg = np.random.randn(D)\n",
    "bq = np.random.randn(D)\n",
    "\n",
    "params = {'We' : We, 'Wf' : Wf, 'Wg' : Wg, 'Wq' : Wq,\n",
    "          'Ue' : Ue, 'Uf' : Uf, 'Ug' : Ug, 'Uq' : Uq,\n",
    "          'be' : be, 'bf' : bf, 'bg' : bg, 'bq' : bq\n",
    "         }\n",
    "\n",
    "# Inputs\n",
    "x = np.random.randn(N, T, d)\n",
    "h_0 = np.random.randn(N, D)\n",
    "s_0 = np.random.randn(N, D)\n",
    "\n",
    "# Test functions\n",
    "fWe = lambda We: lstm_forward(T, x, h_0, s_0, params)[0]\n",
    "fWf = lambda Wf: lstm_forward(T, x, h_0, s_0, params)[0]\n",
    "fWg = lambda Wg: lstm_forward(T, x, h_0, s_0, params)[0]\n",
    "fWq = lambda Wq: lstm_forward(T, x, h_0, s_0, params)[0]\n",
    "\n",
    "fUe = lambda Ue: lstm_forward(T, x, h_0, s_0, params)[0]\n",
    "fUf = lambda Uf: lstm_forward(T, x, h_0, s_0, params)[0]\n",
    "fUg = lambda Ug: lstm_forward(T, x, h_0, s_0, params)[0]\n",
    "fUq = lambda Uq: lstm_forward(T, x, h_0, s_0, params)[0]\n",
    "\n",
    "fbe = lambda be: lstm_forward(T, x, h_0, s_0, params)[0]\n",
    "fbf = lambda bf: lstm_forward(T, x, h_0, s_0, params)[0]\n",
    "fbg = lambda bg: lstm_forward(T, x, h_0, s_0, params)[0]\n",
    "fbq = lambda bq: lstm_forward(T, x, h_0, s_0, params)[0]\n",
    "\n",
    "# Evaluate test functions\n",
    "h, cache_dict = lstm_forward(T, x, h_0, s_0, params)\n",
    "dh = np.random.randn(*h.shape)\n",
    "all_grads = lstm_backward(dh, cache_dict, params)\n",
    "\n",
    "dWe_num = eval_grad(fWe, We, dh)\n",
    "dWf_num = eval_grad(fWf, Wf, dh)\n",
    "dWg_num = eval_grad(fWg, Wg, dh)\n",
    "dWq_num = eval_grad(fWq, Wq, dh)\n",
    "\n",
    "dUe_num = eval_grad(fUe, Ue, dh)\n",
    "dUf_num = eval_grad(fUf, Uf, dh)\n",
    "dUg_num = eval_grad(fUg, Ug, dh)\n",
    "dUq_num = eval_grad(fUq, Uq, dh)\n",
    "\n",
    "dbe_num = eval_grad(fbe, be, dh)\n",
    "dbf_num = eval_grad(fbf, bf, dh)\n",
    "dbg_num = eval_grad(fbg, bg, dh)\n",
    "dbq_num = eval_grad(fbq, bq, dh)\n",
    "\n",
    "print('Error in dWe: {}'.format(rel_error(dWe_num, all_grads['We'])))\n",
    "print('Error in dWf: {}'.format(rel_error(dWf_num, all_grads['Wf'])))\n",
    "print('Error in dWg: {}'.format(rel_error(dWg_num, all_grads['Wg'])))\n",
    "print('Error in dWq: {}'.format(rel_error(dWq_num, all_grads['Wq'])))\n",
    "print('\\n')\n",
    "print('Error in dUe: {}'.format(rel_error(dUe_num, all_grads['Ue'])))\n",
    "print('Error in dUf: {}'.format(rel_error(dUf_num, all_grads['Uf'])))\n",
    "print('Error in dUg: {}'.format(rel_error(dUg_num, all_grads['Ug'])))\n",
    "print('Error in dUq: {}'.format(rel_error(dUq_num, all_grads['Uq'])))\n",
    "print('\\n')\n",
    "print('Error in dbe: {}'.format(rel_error(dbe_num, all_grads['be'])))\n",
    "print('Error in dbf: {}'.format(rel_error(dbf_num, all_grads['bf'])))\n",
    "print('Error in dbg: {}'.format(rel_error(dbg_num, all_grads['bg'])))\n",
    "print('Error in dbq: {}'.format(rel_error(dbq_num, all_grads['bq'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in dU: 7.956108235981939e-10\n",
      "Error in db2: 1.578675630521908e-09\n",
      "Error in dh: 1.0117441876769132e-09\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Check Gradients for the Affine layer \"\"\"\n",
    "\n",
    "np.random.seed(10151)\n",
    "N, T, D, V = 5, 10, 6, 50\n",
    "\n",
    "# Parameters\n",
    "U = np.random.randn(V, D)\n",
    "b2 = np.random.rand(V)\n",
    "\n",
    "# Inputs\n",
    "h = np.random.randn(N, T, D)\n",
    "\n",
    "# Test Functions\n",
    "fU = lambda U: affine_forward(h, U, b2)[0]\n",
    "fb2 = lambda b2: affine_forward(h, U, b2)[0]\n",
    "fh = lambda h: affine_forward(h, U, b2)[0]\n",
    "\n",
    "# Evaluate test functions\n",
    "theta, cache = affine_forward(h, U, b2)\n",
    "dtheta = np.random.randn(*theta.shape)\n",
    "dh, dU, db2 = affine_backward(dtheta, cache)\n",
    "\n",
    "dU_num = eval_grad(fU, U, dtheta)\n",
    "db2_num = eval_grad(fb2, b2, dtheta)\n",
    "dh_num = eval_grad(fh, h, dtheta)\n",
    "\n",
    "print('Error in dU: {}'.format(rel_error(dU_num, dU)))\n",
    "print('Error in db2: {}'.format(rel_error(db2_num, db2)))\n",
    "print('Error in dh: {}'.format(rel_error(dh_num, dh)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec_id'></a>\n",
    "### Training our LSTM\n",
    "We will train our LSTM model on the [Dinosaur Names Dataset](https://github.com/brunoklein99/deep-learning-notes/blob/master/dinos.txt). The training data contains names of real dinosaurs - once trained, we will use the LSTM to sample some made-up dinosaur names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fdd53d9443d44b6a55deda3575ad6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import random\n",
    "\n",
    "N = 512\n",
    "T = 20\n",
    "D = 256\n",
    "\n",
    "loss_freq = 2\n",
    "num_epochs = 300\n",
    "\n",
    "train_file = 'dinos.txt'\n",
    "encoding = 'utf-8'\n",
    "with open(train_file, encoding=encoding) as f:\n",
    "    data = f.read().lower()\n",
    "chars = list(set(data))\n",
    "data_size, V = len(data), len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# Split text into strings of length T+1\n",
    "data_list = [data[i*(T+1):(i+1)*(T+1)].strip() for i in range(len(data)//(T+1))]\n",
    "\n",
    "# Parameters initialization\n",
    "We = np.random.randn(D, D)\n",
    "Wf = np.random.randn(D, D)\n",
    "Wg = np.random.randn(D, D)\n",
    "Wq = np.random.randn(D, D)\n",
    "\n",
    "Ue = np.random.randn(D, V)\n",
    "Uf = np.random.randn(D, V)\n",
    "Ug = np.random.randn(D, V)\n",
    "Uq = np.random.randn(D, V)\n",
    "\n",
    "be = np.zeros(D)\n",
    "bf = np.zeros(D)\n",
    "bg = np.zeros(D)\n",
    "bq = np.zeros(D)\n",
    "\n",
    "U = np.random.randn(V, D)\n",
    "b2 = np.zeros(V)\n",
    "\n",
    "lstm_params = {'We' : We, 'Wf' : Wf, 'Wg' : Wg, 'Wq' : Wq,\n",
    "              'Ue' : Ue, 'Uf' : Uf, 'Ug' : Ug, 'Uq' : Uq,\n",
    "              'be' : be, 'bf' : bf, 'bg' : bg, 'bq' : bq\n",
    "             }\n",
    "\n",
    "# Mems for Adagrad\n",
    "lstm_mems = {'We' : None, 'Wf' : None, 'Wg' : None, 'Wq' : None,\n",
    "              'Ue' : None, 'Uf' : None, 'Ug' : None, 'Uq' : None,\n",
    "              'be' : None, 'bf' : None, 'bg' : None, 'bq' : None\n",
    "             }\n",
    "\n",
    "for param in lstm_params:\n",
    "    lstm_mems[param] = np.zeros_like(lstm_params[param])\n",
    "mU = np.zeros_like(U)\n",
    "mb2 = np.zeros_like(b2)\n",
    "\n",
    "# Other variables' initialization\n",
    "h_0 = np.zeros((N, D))\n",
    "s_0 = np.zeros((N, D))\n",
    "\n",
    "def str_to_idx(st):\n",
    "    idx_arr = np.array([char_to_ix[ch] for ch in st])\n",
    "    return idx_arr\n",
    "lr = 0.1\n",
    "\n",
    "prog_bar = tqdm(total=num_epochs)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start = 0\n",
    "    iter_count = 0\n",
    "    running_loss = 0\n",
    "    while True:\n",
    "        iter_count += 1\n",
    "        batch_str = data_list[start:(start + N)]\n",
    "        batch_idx = [str_to_idx(st) for st in batch_str if len(st) == (T+1)]\n",
    "        batch_size = len(batch_idx)\n",
    "        if batch_size < N:\n",
    "            batch_idx.extend([(T+1)*[0] for i in range(N - batch_size)])\n",
    "        x = np.array([np.eye(V)[indices[0:len(indices)-1]] for indices in batch_idx])\n",
    "        y = np.array([indices[1:] for indices in batch_idx])\n",
    "        mask = np.ones((N, T))\n",
    "        mask[batch_size:,:] = 0\n",
    "\n",
    "        # forward pass\n",
    "        h, cache_dict = lstm_forward(x.shape[1], x, h_0, s_0, lstm_params)\n",
    "        theta, cache = affine_forward(h, U, b2)\n",
    "        loss, dtheta = softmax(theta, y, mask)\n",
    "        running_loss += loss\n",
    "        if iter_count % loss_freq == 0:\n",
    "            prog_bar.set_postfix(epoch='{}/{}'.format(epoch+1, num_epochs), \\\n",
    "                                 loss='{:.3f}'.format(running_loss/(loss_freq*N)))\n",
    "            running_loss = 0\n",
    "        # backprop\n",
    "        dh, dU, db2 = affine_backward(dtheta, cache)\n",
    "        for dz in [dh, dU, db2]: np.clip(dz, -5, 5, out=dz)\n",
    "        all_grads = lstm_backward(dh, cache_dict, lstm_params)\n",
    "        \n",
    "        # update grads\n",
    "        for param in lstm_params:\n",
    "            all_grads[param] = np.clip(all_grads[param], -5, 5)\n",
    "            lstm_mems[param] += all_grads[param]*all_grads[param]\n",
    "            lstm_params[param] += -lr*all_grads[param]/ \\\n",
    "                                np.sqrt(lstm_mems[param] + 1e-8)\n",
    "        \n",
    "        for z, dz, m in zip([U, b2], \n",
    "                            [dU, db2],\n",
    "                            [mU, mb2]):\n",
    "            m += dz*dz\n",
    "            z += -lr*dz / np.sqrt(m + 1e-8)\n",
    "        start += N\n",
    "        if start >= len(data_list): break\n",
    "    prog_bar.update(1)\n",
    "    # Shuffle data\n",
    "    random.shuffle(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights\n",
    "for param in lstm_params:\n",
    "    lstm_params[param].dump(param + '.dat')\n",
    "    lstm_mems[param].dump(param + '_mem.dat')\n",
    "U.dump('U.dat')\n",
    "b2.dump('b2.dat')\n",
    "mU.dump('U_mem.dat')\n",
    "mb2.dump('b2_mem.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample text\n",
    "def sampleText(length=20, seed_ch='T'):\n",
    "    x_t = np.eye(V)[char_to_ix[seed_ch]]\n",
    "    h_prev = np.zeros(D)\n",
    "    s_prev = np.zeros(D)\n",
    "    str_out = seed_ch\n",
    "    for t in range(length):\n",
    "        h_next, s_next, _ = lstm_step_forward(x_t, h_prev, s_prev, lstm_params)\n",
    "        theta = np.matmul(U, h_next) + b2\n",
    "        p = np.exp(theta)/np.sum(np.exp(theta))\n",
    "        idx = np.random.choice(range(V), p=p.ravel())\n",
    "        h_prev = h_next\n",
    "        s_prev = s_next\n",
    "        x_t = np.eye(V)[idx]\n",
    "        str_out += ix_to_char[idx]\n",
    "    return str_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tholin\n",
      "Topsaurus\n",
      "Trosaurucrosauru\n",
      "Tthunntin\n",
      "Totltinptyrus\n",
      "Tylinnnosaurosau\n",
      "Tyrntasaurus\n",
      "Teosaurus\n",
      "Tyrantrosaur\n",
      "Tosaurus\n",
      "Thangeng\n",
      "Tatassaurus\n",
      "Tharaerophybrosa\n",
      "Voteanadus\n",
      "Veintor\n",
      "Venator\n",
      "Vinsaurus\n",
      "Vontioa\n",
      "Venator\n",
      "Venator\n",
      "Venator\n",
      "Veterypasaururaa\n",
      "Venator\n",
      "Venatiror\n",
      "Vetirus\n",
      "Venaroeltratyyps\n",
      "Venelnites\n"
     ]
    }
   ],
   "source": [
    "num_samples = 15\n",
    "for ch in ['t', 'v']:\n",
    "    for i in range(num_samples):\n",
    "        s = sampleText(15, ch)\n",
    "        if not s: continue\n",
    "        pos = s.find('\\n')\n",
    "        if pos != -1: s = s[0:pos]\n",
    "        if len(s) > 5: print(s.capitalize())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
