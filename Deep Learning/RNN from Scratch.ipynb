{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/RNN Diagram.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-np.clip(x, -500, 500)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation for RNN Units\n",
    "An RNN Unit at time-step $t$ takes as input: <br/>\n",
    "* a minibatch of 'words' denoted by $x^{(t)}$, of dimensions $N \\times d$, and <br/>\n",
    "* the 'hidden-state' vector $h^{(t-1)}$ from the previous unit, of dimensions $N \\times D_h$.\n",
    "\n",
    "**Note: $d$ and $D_h$ are hyper-parmaters, i.e. we _chose_ to represent each hidden state using a vector of length $D_h$ and we _chose_ to use 'word embedding' vectors of length $d$.**\n",
    "\n",
    "The code below implements a single RNN Unit's computation. The output is the 'hidden-state' vector $h^{(t)}$ for this unit, of dimensions $N \\times D_h$. (In this notebook, $h^{(t)}$ for time-step $t$ is always referred to as $h\\_next$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_forward(x_t, h_prev, Wh, We, b1):\n",
    "    h_next = sigmoid(np.matmul(h_prev, Wh.T) + np.matmul(x_t, We.T) + b1)\n",
    "    cache = h_prev, h_next, x_t\n",
    "    return h_next, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An RNN Unit depends on the previous RNN Unit's hidden-state (this is not different from any plain feedforward network). Therefore we sequentially run the $rnn\\_step\\_forward$ method implemented above, for each time step.\n",
    "\n",
    "**Note: One crucial difference from a plain feedforward network is that each RNN Unit uses the same parameters $W_h$, $W_e$, and $b_1$. This point of difference will have a significant bearing on how we backprop through an RNN.**\n",
    "\n",
    "The code below implements the forward pass through an RNN. We are given as inputs:\n",
    "* a minibatch of 'word sequences' denoted by $x$, of dimensions $N \\times T \\times d$, where $N$ is the numnber of minibatches and $T$ is the length of each sequence,\n",
    "* an initial state vector denoted by $h^{(0)}$ of dimensions $N \\times D_h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(T, x, h_0, Wh, We, b1):\n",
    "    N, T, d = x.shape\n",
    "    _, Dh = h_0.shape\n",
    "    h = np.zeros((N, T, Dh))\n",
    "    h_prev = h_0\n",
    "    cache_dict = {}\n",
    "    for t in range(T):\n",
    "        h[:, t, :],  cache_step = rnn_step_forward(x[:,t,:], h_prev, Wh, We, b1) \n",
    "        h_prev = h[:, t, :]\n",
    "        cache_dict.update({t : cache_step})\n",
    "    return h, cache_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each time-step $t$, we are given as inputs:\n",
    "* the cache for this time-step saved during our forward pass - cache stores $h^{(t)}$ and $h^{(t-1)}$,\n",
    "* the gradient of total loss $J$ with respect to $h^{(t)}$, denoted by $dh\\_next$, of dimensions $N \\times D_h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_backward(dh_next, cache, Wh):\n",
    "    h_prev, h_next, x_t = cache\n",
    "    dsigmoid = h_next*(1 - h_next)\n",
    "    interim_dot_prod = dh_next*dsigmoid       # This will be used in all equations below\n",
    "    dWh_step = np.matmul(interim_dot_prod.T, h_prev)\n",
    "    dWe_step = np.matmul(interim_dot_prod.T, x_t)\n",
    "    db1_step = np.sum(interim_dot_prod, axis=0)\n",
    "    dh_prev = np.matmul(interim_dot_prod, Wh)\n",
    "    return  dWh_step, dWe_step, db1_step, dh_prev "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(dh, cache_dict, Wh, We, b1):\n",
    "    N, T, Dh = dh.shape\n",
    "    dWh = np.zeros_like(Wh)\n",
    "    dWe = np.zeros_like(We)\n",
    "    db1 = np.zeros_like(b1)\n",
    "    dh_next = np.zeros((N, Dh))\n",
    "    for t in range(T, 0, -1):\n",
    "        dh_next += dh[:, t-1, :]\n",
    "        dWh_step, dWe_step, db1_step, dh_prev = rnn_step_backward(dh_next, cache_dict[t-1], Wh)\n",
    "        dh_next = dh_prev\n",
    "        dWh += dWh_step\n",
    "        dWe += dWe_step\n",
    "        db1 += db1_step\n",
    "    return dWh, dWe, db1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation for the Affine Layer \n",
    "Atop each RNN Unit, sits an Affine layer which takes the vector $h^{(t)}$ as input, applies an Affine transformation, and computes the Softmax Probability. The parameters of this layer are $U \\space (Dim: D_{h} \\times V )$ and $b_2 \\space (Dim: V \\times 1)$. \n",
    "\n",
    "We do not have to implement a separate Affine layer for each time-step. Unlike in the case of an RNN Unit where the computation inside it depended on the output of its previous unit, the Affine computations at each time-step are independent of each other. Therefore, once we have computed $h^{(t)}$ for each time-step, we will perform the Affine computation for ALL $T$ time-steps in one go, taking into impact the contribution from ALL mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(h, U, b2):\n",
    "    N, T, Dh = h.shape\n",
    "    V = b2.shape[0]\n",
    "    theta = (np.matmul(h.reshape(N*T, Dh), U.T) + b2).reshape(N, T, V)\n",
    "    cache = U, b2, h\n",
    "    return theta, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_backward(dtheta, cache):\n",
    "    U, b2, h = cache\n",
    "    Dh = U.shape[1]\n",
    "    N, T, V = dtheta.shape\n",
    "    dh = np.matmul(dtheta.reshape(N*T, V), U).reshape(N, T, Dh)\n",
    "    dU = np.matmul((dtheta.reshape(N*T, V).T), h.reshape(N*T, Dh))\n",
    "    db2 = dtheta.sum(axis=(0,1))\n",
    "    return dh, dU, db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation for the Softmax Layer\n",
    "We will compute probabilities for all $T$ sequences in a minibatch, over all $N$ minibatches, in one go. This follows from similar reasoning as I described for the Affine layer above.\n",
    "\n",
    "**Inputs:**\n",
    "* Matrix $\\theta$ of dimensions $N \\times T \\times V$ which stores the output of Affine Layers, and\n",
    "* Matrix $y$ of dimensions $N \\times T$ which stores the index in Vocabulary of the true 'word' for each time-step, for each minibatch.\n",
    "\n",
    "**Outputs:**\n",
    "* Loss over all minibatches (a single floating point number), and\n",
    "* Matrix $dtheta$ of same dimensions as $\\theta$, and which stores gradients of Loss w.r.t $\\theta$.\n",
    "\n",
    "**Notes:** \n",
    "* I have directly lifted the $softmax\\_loss$ function from the starter code of Assignment 3 of Stanford's CS231n's [Winter 2016 edition](http://cs231n.stanford.edu/2016/).\n",
    "* This version of $softmax\\_loss$ uses a 'mask', an array of dimensions $N \\times T$ which indicates which time-steps in a minibatch should not be counted towards the loss. This is used to handle sequences whose length is less than $T$ - we pad them with zeros (in my implementation) to increase their length to $T$, which makes for easy code elsewhere. \n",
    "* I have previously discussed maths for backprop through a Softmax layer in a [blog post](https://talwarabhimanyu.github.io/blog/2017/05/20/softmax-backprop). You may refer to $Equation \\space 1.3$ in that post which derives the gradient of Loss w.r.t $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(theta, y, mask):\n",
    "    N, T, V = theta.shape\n",
    "    theta_flat = theta.reshape(N*T, V)\n",
    "    y_flat = y.reshape(N*T)\n",
    "    mask_flat = mask.reshape(N*T)\n",
    "    \n",
    "    probs = np.exp(theta_flat - np.max(theta_flat, axis=1, keepdims=True))\n",
    "    probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "    loss = -np.sum(mask_flat * np.log(probs[np.arange(N * T), y_flat])) / N\n",
    "    dtheta_flat = probs.copy()\n",
    "    dtheta_flat[np.arange(N * T), y_flat] -= 1\n",
    "    dtheta_flat /= N\n",
    "    dtheta_flat *= mask_flat[:, None]\n",
    "    \n",
    "    dtheta = dtheta_flat.reshape(N, T, V)\n",
    "    return loss, dtheta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Evaluation of Gradients to Check Correctness of our Implementation\n",
    "If you are familiar with how to numerically check gradients for a network, you can skip this section and move on to [Training our RNN](#sec_id)\n",
    "\n",
    "Below, the function $eval\\_grad$ evaluates the gradient of a given function $f$ at a point $x$. This point $x$ can be multidimensional, for example I will use the $2D$ matrix $W_h$ as a 'point'. The 'gradient' is basically the change in Loss due to an infinitesimally small perturbation to the point $x$.\n",
    "\n",
    "Notice in the code below that I have multiplied by $dh$ to calculate the gradient. This is because we are going to be passing $rnn\\_forward$ and $rnn\\_step\\_forward$ for the argument $f$. Both these functions return the vector $h$ and not the scalar Loss which we need to compute the gradient w.r.t point $x$. Therefore we need to multiple by $dh$ to get our gradient, which is what we pass for the argument $df$.\n",
    "\n",
    "**Note: Below, I have used the numerical gradient evaluation functions provided for assignments of Stanford's course CS321n, _\"Convolutional Neural Networks for Visual Recognition\"_. Specifically, I have used code from the [Winter 2016 edition](http://cs231n.stanford.edu/2016/).  **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_grad(f, x, df):\n",
    "    grad = np.zeros_like(x)\n",
    "    epsilon = 1e-5\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        orig_val = x[idx]\n",
    "        x[idx] = orig_val + epsilon\n",
    "        fwd_fx = f(x)\n",
    "        x[idx] = orig_val - epsilon\n",
    "        bck_fx = f(x)\n",
    "        grad[idx] = np.sum((fwd_fx - bck_fx)*df/(epsilon*2))\n",
    "        x[idx] = orig_val\n",
    "        it.iternext()\n",
    "    return grad\n",
    "def rel_error(x, y):\n",
    "    # Returns relative error\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in dWh: 3.699756155582621e-11\n",
      "Error in dWe: 2.8456523039254035e-10\n",
      "Error in db1: 2.1963488162818666e-11\n",
      "Error in dh_prev: 2.3951187440793865e-11\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Check Gradients for single RNN Unit \"\"\"\n",
    "\n",
    "np.random.seed(10151)\n",
    "N, Dh, d, V = 2, 5, 3, 50\n",
    "# Parameters\n",
    "Wh = np.random.randn(Dh, Dh)\n",
    "b1 = np.random.randn(Dh)\n",
    "We = np.random.randn(Dh, d)\n",
    "\n",
    "# Inputs\n",
    "x_t = np.random.randn(N, d)\n",
    "h_prev = np.random.randn(N, Dh)\n",
    "\n",
    "# Test functions\n",
    "fWh = lambda Wh: rnn_step_forward(x_t, h_prev, Wh, We, b1)[0]\n",
    "fWe = lambda We: rnn_step_forward(x_t, h_prev, Wh, We, b1)[0]\n",
    "fb1 = lambda b1: rnn_step_forward(x_t, h_prev, Wh, We, b1)[0]\n",
    "fh_prev = lambda h_prev: rnn_step_forward(x_t, h_prev, Wh, We, b1)[0]\n",
    "\n",
    "# Evaluate test functions\n",
    "h_next, cache_step = rnn_step_forward(x_t, h_prev, Wh, We, b1)\n",
    "dh_next = np.random.randn(*h_next.shape)\n",
    "dWh, dWe, db1, dh_prev = rnn_step_backward(dh_next, cache_step, Wh)\n",
    "dWh_num = eval_grad(fWh, Wh, dh_next)\n",
    "dWe_num = eval_grad(fWe, We, dh_next)\n",
    "db1_num = eval_grad(fb1, b1, dh_next)\n",
    "dh_prev_num = eval_grad(fh_prev, h_prev, dh_next)\n",
    "print('Error in dWh: {}'.format(rel_error(dWh_num, dWh)))\n",
    "print('Error in dWe: {}'.format(rel_error(dWe_num, dWe)))\n",
    "print('Error in db1: {}'.format(rel_error(db1_num, db1)))\n",
    "print('Error in dh_prev: {}'.format(rel_error(dh_prev_num, dh_prev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in dWh: 9.831792282221475e-10\n",
      "Error in dWe: 1.0017182281175608e-10\n",
      "Error in db1: 9.769693584166271e-11\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Check Gradients for the entire RNN \"\"\"\n",
    "\n",
    "np.random.seed(10151)\n",
    "T, N, Dh, d, V = 10, 2, 5, 3, 50\n",
    "# Parameters\n",
    "Wh = np.random.randn(Dh, Dh)\n",
    "b1 = np.random.randn(Dh)\n",
    "We = np.random.randn(Dh, d)\n",
    "\n",
    "# Inputs\n",
    "x = np.random.randn(N, T, d)\n",
    "h_0 = np.random.randn(N, Dh)\n",
    "\n",
    "# Test functions\n",
    "fWh = lambda Wh: rnn_forward(T, x, h_0, Wh, We, b1)[0]\n",
    "fWe = lambda We: rnn_forward(T, x, h_0, Wh, We, b1)[0]\n",
    "fb1 = lambda b1: rnn_forward(T, x, h_0, Wh, We, b1)[0]\n",
    "\n",
    "# Evaluate test functions\n",
    "h, cache_dict = rnn_forward(T, x, h_0, Wh, We, b1)\n",
    "dh = np.random.randn(*h.shape)\n",
    "dWh, dWe, db1 = rnn_backward(dh, cache_dict, Wh, We, b1)\n",
    "dWh_num = eval_grad(fWh, Wh, dh)\n",
    "dWe_num = eval_grad(fWe, We, dh)\n",
    "db1_num = eval_grad(fb1, b1, dh)\n",
    "print('Error in dWh: {}'.format(rel_error(dWh_num, dWh)))\n",
    "print('Error in dWe: {}'.format(rel_error(dWe_num, dWe)))\n",
    "print('Error in db1: {}'.format(rel_error(db1_num, db1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in dU: 7.956108235981939e-10\n",
      "Error in db2: 1.578675630521908e-09\n",
      "Error in dh: 1.0117441876769132e-09\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Check Gradients for the Affine layer \"\"\"\n",
    "\n",
    "np.random.seed(10151)\n",
    "N, T, Dh, V = 5, 10, 6, 50\n",
    "\n",
    "# Parameters\n",
    "U = np.random.randn(V, Dh)\n",
    "b2 = np.random.rand(V)\n",
    "\n",
    "# Inputs\n",
    "h = np.random.randn(N, T, Dh)\n",
    "\n",
    "# Test Functions\n",
    "fU = lambda U: affine_forward(h, U, b2)[0]\n",
    "fb2 = lambda b2: affine_forward(h, U, b2)[0]\n",
    "fh = lambda h: affine_forward(h, U, b2)[0]\n",
    "\n",
    "# Evaluate test functions\n",
    "theta, cache = affine_forward(h, U, b2)\n",
    "dtheta = np.random.randn(*theta.shape)\n",
    "dh, dU, db2 = affine_backward(dtheta, cache)\n",
    "\n",
    "dU_num = eval_grad(fU, U, dtheta)\n",
    "db2_num = eval_grad(fb2, b2, dtheta)\n",
    "dh_num = eval_grad(fh, h, dtheta)\n",
    "\n",
    "print('Error in dU: {}'.format(rel_error(dU_num, dU)))\n",
    "print('Error in db2: {}'.format(rel_error(db2_num, db2)))\n",
    "print('Error in dh: {}'.format(rel_error(dh_num, dh)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec_id'></a>\n",
    "### Training our RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iteration 100, Running Loss 2.68\n",
      "Epoch 1 Iteration 200, Running Loss 1.63\n",
      "Epoch 1 Iteration 300, Running Loss 1.46\n",
      "Epoch 1 Iteration 400, Running Loss 1.39\n",
      "Epoch 1 Iteration 500, Running Loss 1.34\n",
      "Epoch 1 Iteration 600, Running Loss 1.31\n",
      "Epoch 1 Iteration 700, Running Loss 1.28\n",
      "Epoch 2 Iteration 100, Running Loss 2.47\n",
      "Epoch 2 Iteration 200, Running Loss 1.26\n",
      "Epoch 2 Iteration 300, Running Loss 1.24\n",
      "Epoch 2 Iteration 400, Running Loss 1.21\n",
      "Epoch 2 Iteration 500, Running Loss 1.20\n",
      "Epoch 2 Iteration 600, Running Loss 1.19\n",
      "Epoch 2 Iteration 700, Running Loss 1.18\n",
      "Epoch 3 Iteration 100, Running Loss 2.29\n",
      "Epoch 3 Iteration 200, Running Loss 1.17\n",
      "Epoch 3 Iteration 300, Running Loss 1.17\n",
      "Epoch 3 Iteration 400, Running Loss 1.15\n",
      "Epoch 3 Iteration 500, Running Loss 1.14\n",
      "Epoch 3 Iteration 600, Running Loss 1.14\n",
      "Epoch 3 Iteration 700, Running Loss 1.12\n",
      "Epoch 4 Iteration 100, Running Loss 2.19\n",
      "Epoch 4 Iteration 200, Running Loss 1.13\n",
      "Epoch 4 Iteration 300, Running Loss 1.13\n",
      "Epoch 4 Iteration 400, Running Loss 1.11\n",
      "Epoch 4 Iteration 500, Running Loss 1.10\n",
      "Epoch 4 Iteration 600, Running Loss 1.10\n",
      "Epoch 4 Iteration 700, Running Loss 1.09\n",
      "Epoch 5 Iteration 100, Running Loss 2.12\n",
      "Epoch 5 Iteration 200, Running Loss 1.09\n",
      "Epoch 5 Iteration 300, Running Loss 1.10\n",
      "Epoch 5 Iteration 400, Running Loss 1.08\n",
      "Epoch 5 Iteration 500, Running Loss 1.07\n",
      "Epoch 5 Iteration 600, Running Loss 1.08\n",
      "Epoch 5 Iteration 700, Running Loss 1.06\n",
      "Epoch 6 Iteration 100, Running Loss 2.08\n",
      "Epoch 6 Iteration 200, Running Loss 1.07\n",
      "Epoch 6 Iteration 300, Running Loss 1.08\n",
      "Epoch 6 Iteration 400, Running Loss 1.06\n",
      "Epoch 6 Iteration 500, Running Loss 1.05\n",
      "Epoch 6 Iteration 600, Running Loss 1.06\n",
      "Epoch 6 Iteration 700, Running Loss 1.04\n",
      "Epoch 7 Iteration 100, Running Loss 2.04\n",
      "Epoch 7 Iteration 200, Running Loss 1.05\n",
      "Epoch 7 Iteration 300, Running Loss 1.06\n",
      "Epoch 7 Iteration 400, Running Loss 1.04\n",
      "Epoch 7 Iteration 500, Running Loss 1.03\n",
      "Epoch 7 Iteration 600, Running Loss 1.04\n",
      "Epoch 7 Iteration 700, Running Loss 1.03\n",
      "Epoch 8 Iteration 100, Running Loss 2.00\n",
      "Epoch 8 Iteration 200, Running Loss 1.03\n",
      "Epoch 8 Iteration 300, Running Loss 1.04\n",
      "Epoch 8 Iteration 400, Running Loss 1.02\n",
      "Epoch 8 Iteration 500, Running Loss 1.02\n",
      "Epoch 8 Iteration 600, Running Loss 1.02\n",
      "Epoch 8 Iteration 700, Running Loss 1.01\n",
      "Epoch 9 Iteration 100, Running Loss 1.97\n",
      "Epoch 9 Iteration 200, Running Loss 1.01\n",
      "Epoch 9 Iteration 300, Running Loss 1.03\n",
      "Epoch 9 Iteration 400, Running Loss 1.01\n",
      "Epoch 9 Iteration 500, Running Loss 1.01\n",
      "Epoch 9 Iteration 600, Running Loss 1.01\n",
      "Epoch 9 Iteration 700, Running Loss 1.00\n",
      "Epoch 10 Iteration 100, Running Loss 1.95\n",
      "Epoch 10 Iteration 200, Running Loss 1.00\n",
      "Epoch 10 Iteration 300, Running Loss 1.01\n",
      "Epoch 10 Iteration 400, Running Loss 1.00\n",
      "Epoch 10 Iteration 500, Running Loss 0.99\n",
      "Epoch 10 Iteration 600, Running Loss 1.00\n",
      "Epoch 10 Iteration 700, Running Loss 0.99\n",
      "Epoch 11 Iteration 100, Running Loss 1.92\n",
      "Epoch 11 Iteration 200, Running Loss 0.99\n",
      "Epoch 11 Iteration 300, Running Loss 1.00\n",
      "Epoch 11 Iteration 400, Running Loss 0.98\n",
      "Epoch 11 Iteration 500, Running Loss 0.98\n",
      "Epoch 11 Iteration 600, Running Loss 0.99\n",
      "Epoch 11 Iteration 700, Running Loss 0.98\n",
      "Epoch 12 Iteration 100, Running Loss 1.90\n",
      "Epoch 12 Iteration 200, Running Loss 0.98\n",
      "Epoch 12 Iteration 300, Running Loss 0.99\n",
      "Epoch 12 Iteration 400, Running Loss 0.97\n",
      "Epoch 12 Iteration 500, Running Loss 0.97\n",
      "Epoch 12 Iteration 600, Running Loss 0.98\n",
      "Epoch 12 Iteration 700, Running Loss 0.97\n",
      "Epoch 13 Iteration 100, Running Loss 1.88\n",
      "Epoch 13 Iteration 200, Running Loss 0.97\n",
      "Epoch 13 Iteration 300, Running Loss 0.98\n",
      "Epoch 13 Iteration 400, Running Loss 0.97\n",
      "Epoch 13 Iteration 500, Running Loss 0.96\n",
      "Epoch 13 Iteration 600, Running Loss 0.97\n",
      "Epoch 13 Iteration 700, Running Loss 0.96\n",
      "Epoch 14 Iteration 100, Running Loss 1.87\n",
      "Epoch 14 Iteration 200, Running Loss 0.96\n",
      "Epoch 14 Iteration 300, Running Loss 0.98\n",
      "Epoch 14 Iteration 400, Running Loss 0.96\n",
      "Epoch 14 Iteration 500, Running Loss 0.96\n",
      "Epoch 14 Iteration 600, Running Loss 0.96\n",
      "Epoch 14 Iteration 700, Running Loss 0.95\n",
      "Epoch 15 Iteration 100, Running Loss 1.85\n",
      "Epoch 15 Iteration 200, Running Loss 0.95\n",
      "Epoch 15 Iteration 300, Running Loss 0.97\n",
      "Epoch 15 Iteration 400, Running Loss 0.95\n",
      "Epoch 15 Iteration 500, Running Loss 0.95\n",
      "Epoch 15 Iteration 600, Running Loss 0.96\n",
      "Epoch 15 Iteration 700, Running Loss 0.94\n"
     ]
    }
   ],
   "source": [
    "N = 64\n",
    "T = 50\n",
    "Dh = 64\n",
    "\n",
    "train_file = 'Berkshire Letters.txt'\n",
    "with open(train_file, 'r') as f:\n",
    "    data = f.read()\n",
    "chars = list(set(data))\n",
    "data_size, V = len(data), len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# Parameters initialization\n",
    "Wh = np.random.randn(Dh, Dh)\n",
    "b1 = np.zeros(Dh)\n",
    "We = np.random.randn(Dh, V)\n",
    "U = np.random.randn(V, Dh)\n",
    "b2 = np.zeros(V)\n",
    "\n",
    "# Mem for Adagrad\n",
    "mWh, mWe, mU = np.zeros_like(Wh), np.zeros_like(We), np.zeros_like(U)\n",
    "mb1, mb2 = np.zeros_like(b1), np.zeros_like(b2)\n",
    "\n",
    "# Other variables' initialization\n",
    "h_0 = np.zeros((N, Dh))\n",
    "\n",
    "def str_to_idx(st):\n",
    "    idx_arr = np.array([char_to_ix[ch] for ch in st])\n",
    "    return idx_arr\n",
    "lr = 0.01\n",
    "loss_freq = 100\n",
    "num_iter = 0\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    start = 0\n",
    "    iter_count = 0\n",
    "    running_loss = 0\n",
    "    while True:\n",
    "        num_iter += 1\n",
    "        iter_count += 1\n",
    "        batch_str = [data[(start + i*T):(start + (i+1)*T) + 1] for i in range(N)]\n",
    "        batch_idx = [str_to_idx(st) for st in batch_str if len(st) == (T+1)]\n",
    "        batch_size = len(batch_idx)\n",
    "        if batch_size < N:\n",
    "            batch_idx.extend([(T+1)*[0] for i in range(N - batch_size)])\n",
    "        x = np.array([np.eye(V)[indices[0:len(indices)-1]] for indices in batch_idx])\n",
    "        y = np.array([indices[1:] for indices in batch_idx])\n",
    "        mask = np.ones((N, T))\n",
    "        mask[batch_size:,:] = 0\n",
    "        \n",
    "        # forward pass\n",
    "        h, cache_dict = rnn_forward(x.shape[1], x, h_0, Wh, We, b1)\n",
    "        theta, cache = affine_forward(h, U, b2)\n",
    "        loss, dtheta = softmax(theta, y, mask)\n",
    "        running_loss += loss\n",
    "        if iter_count % loss_freq == 0:\n",
    "            print('Epoch {} Iteration {}, Running Loss {:.2f}'.format(epoch + 1, iter_count, \\\n",
    "                                                                      running_loss/loss_freq/N))\n",
    "            running_loss = 0\n",
    "        # backprop\n",
    "        dh, dU, db2 = affine_backward(dtheta, cache)\n",
    "        for dz in [dh, dU, db2]: np.clip(dz, -5, 5, out=dz)\n",
    "        dWh, dWe, db1 = rnn_backward(dh, cache_dict, Wh, We, b1)\n",
    "        for dz in [dWh, dWe, db1]: np.clip(dz, -5, 5, out=dz)\n",
    "        \n",
    "        # update grads\n",
    "        for z, dz, m in zip([Wh, We, U, b1, b2], \n",
    "                            [dWh, dWe, dU, db1, db2],\n",
    "                            [mWh, mWe, mU, mb1, mb2]):\n",
    "            m += dz*dz\n",
    "            z += -lr*dz / np.sqrt(m + 1e-8)\n",
    "        start += N*T\n",
    "        if start >= data_size: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample text\n",
    "def sampleText(length=100, seed_ch='T'):\n",
    "    x_t = np.eye(V)[char_to_ix[seed_ch]]\n",
    "    h_prev = np.zeros(Dh)\n",
    "    str_out = seed_ch\n",
    "    for t in range(length):\n",
    "        h_next = sigmoid(np.matmul(Wh, h_prev) + np.matmul(We, x_t) + b1)\n",
    "        theta = np.matmul(U, h_next) + b2\n",
    "        p = np.exp(theta)/np.sum(np.exp(theta))\n",
    "        idx = np.random.choice(range(V), p=p.ravel())\n",
    "        h_prev = h_next\n",
    "        x_t = np.eye(V)[idx]\n",
    "        str_out += ix_to_char[idx]\n",
    "    print(str_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T\u0000e\u0000r\u0000e\u0000l\u0000o\u0000s\u0000 \u0000-\u0000u\u00000\u0000d\u0000 \u0000c\u0000e\u0000d\u0000t\u0000e\u0000s\u0000 \u0000e\u0000p\u0000t\u0000h\u0000e\u0000 \u0000c\u0000o\u0000r\u0000i\u0000l\u0000e\u0000 \u0000h\u0000i\u0000b\u0000e\u0000 \u0000t\u0000h\u0000e\u0000c\u0000e\u0000 \u0000o\u0000m\u0000i\u0000l\u0000g\u0000 \u0000A\u0000s\u0000e\u0000 \u0000h\u0000i\u0000n\u0000 \u0000t\u0000h\u0000a\u0000r\u0000 \u0000w\u0000i\u0000n\u00004\u0000u\u0000r\u0000r\u0000i\u0000t\u0000U\u0000v\u0000i\u0000n\u0000t\u0000e\u0000e\u0000d\u0000 \u0000f\u0000 \u0000m\u0000e\u0000q\u0000l\u0000e\u0000r\u0000s\u0000d\u0000y\u0000 \u0000s\u0000h\u0000e\u0000 \u0000w\u0000o\u0000r\u0000o\u0000c\u0000e\u0000 \u0000d\u0000 \u0000y\u0000f\u0000e\u0000 \u0000a\u0000s\u0000c\u0000s\u0000a\u0000o\u0000w\u0000h\u0000i\u0000i\u0000l\u0000u\u0000i\u0000l\u0000e\u0000y\u0000.\u0000s\u0000 \u0000b\u0000B\u0000e\u0000m\u0000o\u0000w\u0000 \u0000o\u0000r\u0000,\u0000 \u0000n\u0000o\u0000.\u0000 \u00008\u0000H\u0000 \u0000a\u0000n\u0000.\u0000 \u0000.\u0000s\u0000.\u0000 \u0000.\u0000 \u0000.\u0000 \u0000.\u0000 \u0000.\u0000 \u0000.\u0000 \u0000.\u0000 \u0000T\u0000h\u0000e\u0000r\u0000 \u0000o\u0000e\u0000q\u0000u\u0000e\u0000n\u0000t\u0000e\u0000 \u0000t\u0000o\u0000o\u0000\n",
      "\u0000,\u0000 \u0000f\u0000 \u0000t\u0000e\u0000 \u0000o\u0000 \u0000a\u0000r\u0000a\u0000n\u0000g\u0000 \u0000b\u0000e\u0000i\u0000t\u0000e\u0000 \u0000s\u0000I\u00002\u0000 \u0000I\u0000u\u0000g\u00007\u00007\u0000\t\u0000 \u0000,\u0000 \u0000P\u0000o\u0000s\u0000 \u00002\u0000 \u0000a\u0000n\u0000v\u0000e\u0000s\u0000e\u0000p\u0000t\u0000r\u0000i\u0000c\u0000i\u0000n\u0000e\u0000e\u0000d\u0000 \u0000.\u0000 \u0000\t\u0000t\u0000 \u0000.\u0000 \u0000.\u0000 \u0000,\u0000 \u0000t\u0000h\u0000e\u0000r\u0000 \u0000a\u0000b\u0000n\u0000t\u0000k\u0000a\u0000n\u0000 \u00003\u0000n\u0000 \u0000o\u0000n\u0000 \u0000g\u0000y\u0000 \u0000m\u0000e\u0000w\u0000o\u0000y\u0000a\u00006\u0000d\u0000 \u0000.\u0000W\u0000.\u0000 \u0000.\u0000 \u0000.\u0000 \u0000.\u0000 \u0000O\u0000l\u0000 \u0000n\u0000o\u0000u\u0000s\u0000i\u0000o\u0000t\u0000 \u0000N\u0000i\u0000p\u0000C\u0000 \u0000e\u0000n\u0000 \u0000a\u0000n\u0000k\u0000e\u0000v\u0000t\u0000o\u0000r\u0000 \u0000.\u00009\u0000 \u0000i\u0000n\u0000g\u0000 \u0000h\u0000o\u0000c\u0000 \u0000u\u0000r\u0000a\u0000t\u0000 \u0000;\u0000 \u0000h\u0000e\u0000 \u0000\t\u0000m\u0000T\u0000h\u0000o\u0000n\u0000N\u0000e\u0000 \u0000c\u0000o\u0000n\u0000 \u0000e\u0000n\u0000 \u0000i\u0000t\u0000 \u0000t\u0000o\u0000 \u0000a\u0000r\u0000o\u0000m\u0000e\u0000\t\u0000\t\u0000\n",
      "\u0000\n",
      "\u0000B\u0000a\u0000s\u0000a\u0000g\u0000c\u0000 \u0000a\u0000l\u0000e\u0000l\u0000o\u0000m\u0000a\u0000c\u0000e\u0000 \u0000I\u0000g\u0000 \u0000d\u0000u\u0000s\u0000x\u0000a\u0000l\u0000l\u0000-\u0000d\u0000i\u0000n\u0000e\u0000a\u0000r\u0000s\u0000k\u0000e\u0000 \u0000y\u0000f\u0000a\u0000r\u0000 \u0000t\u0000h\u0000e\u0000 \u0000 \u0000t\u0000i\u0000c\u0000i\u0000r\u0000i\u0000t\u0000h\u0000e\u0000u\u0000l\u0000r\u0000 \u0000u\u0000c\u0000t\u0000o\u0000a\u0000n\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000O\u0000r\u0000 \u0000a\u0000n\u0000 \u0000t\u0000o\u0000 \u0000p\u0000o\u0000r\u0000i\u0000b\u0000e\u0000 \u0000c\u0000a\u0000p\u0000i\u0000l\u0000l\u0000i\u0000b\u0000e\u0000a\u0000m\u0000e\u0000 \u0000n\u0000d\u0000a\u0000r\u0000 \u0000y\u0000a\u0000s\u0000l\u0000t\u00008\u0000t\u0000h\u0000a\u0000d\u0000 \u0000c\u0000i\u0000r\u0000e\u0000 \u0000c\u0000a\u0000c\n"
     ]
    }
   ],
   "source": [
    "sampleText(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.zeros(5)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
