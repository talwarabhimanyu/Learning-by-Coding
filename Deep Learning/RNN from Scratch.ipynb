{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/RNN Diagram.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-np.clip(x, -500, 500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An RNN Unit at time-step $t$ takes as input: <br/>\n",
    "* a minibatch of 'words' denoted by $x^{(t)}$, of dimensions $N \\times d$, and <br/>\n",
    "* the 'hidden-state' vector $h^{(t-1)}$ from the previous unit, of dimensions $N \\times D_h$.\n",
    "\n",
    "**Note: $d$ and $D_h$ are hyper-parmaters, i.e. we _chose_ to represent each hidden state using a vector of length $D_h$ and we _chose_ to use 'word embedding' vectors of length $d$.**\n",
    "\n",
    "The code below implements a single RNN Unit's computation. The output is the 'hidden-state' vector $h^{(t)}$ for this unit, of dimensions $N \\times D_h$. (In this notebook, $h^{(t)}$ for time-step $t$ is always referred to as $h\\_next$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_forward(x_t, h_prev, Wh, We, b1):\n",
    "    h_next = sigmoid(np.matmul(h_prev, Wh.T) + np.matmul(x_t, We.T) + b1)\n",
    "    cache = h_prev, h_next, x_t\n",
    "    return h_next, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An RNN Unit depends on the previous RNN Unit's hidden-state (this is not different from any plain feedforward network). Therefore we sequentially run the $rnn\\_step\\_forward$ method implemented above, for each time step.\n",
    "\n",
    "**Note: One crucial difference from a plain feedforward network is that each RNN Unit uses the same parameters $W_h$, $W_e$, and $b_1$. This point of difference will have a significant bearing on how we backprop through an RNN.**\n",
    "\n",
    "The code below implements the forward pass through an RNN. We are given as inputs:\n",
    "* a minibatch of 'word sequences' denoted by $x$, of dimensions $N \\times T \\times d$, where $N$ is the numnber of minibatches and $T$ is the length of each sequence,\n",
    "* an initial state vector denoted by $h^{(0)}$ of dimensions $N \\times D_h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(T, x, h_0, Wh, We, b1):\n",
    "    N, T, d = x.shape\n",
    "    _, Dh = h_0.shape\n",
    "    h = np.zeros((N, T, Dh))\n",
    "    h_prev = h_0\n",
    "    cache_dict = {}\n",
    "    for t in range(T):\n",
    "        h[:, t, :],  cache_step = rnn_step_forward(x[:,t,:], h_prev, Wh, We, b1) \n",
    "        h_prev = h[:, t, :]\n",
    "        cache_dict.update({t : cache_step})\n",
    "    return h, cache_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each time-step $t$, we are given as inputs:\n",
    "* the cache for this time-step saved during our forward pass - cache stores $h^{(t)}$ and $h^{(t-1)}$,\n",
    "* the gradient of total loss $J$ with respect to $h^{(t)}$, denoted by $dh\\_next$, of dimensions $N \\times D_h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_backward(dh_next, cache, Wh):\n",
    "    h_prev, h_next, x_t = cache\n",
    "    dsigmoid = h_next*(1 - h_next)\n",
    "    interim_dot_prod = dh_next*dsigmoid       # This will be used in all equations below\n",
    "    dWh_step = np.matmul(interim_dot_prod.T, h_prev)\n",
    "    dWe_step = np.matmul(interim_dot_prod.T, x_t)\n",
    "    db1_step = np.sum(interim_dot_prod, axis=0)\n",
    "    dh_prev = np.matmul(interim_dot_prod, Wh)\n",
    "    return  dWh_step, dWe_step, db1_step, dh_prev "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(dh, cache_dict, Wh, We, b1):\n",
    "    N, T, Dh = dh.shape\n",
    "    dWh = np.zeros_like(Wh)\n",
    "    dWe = np.zeros_like(We)\n",
    "    db1 = np.zeros_like(b1)\n",
    "    dh_next = np.zeros((N, Dh))\n",
    "    for t in range(T, 0, -1):\n",
    "        dh_next += dh[:, t-1, :]\n",
    "        dWh_step, dWe_step, db1_step, dh_prev = rnn_step_backward(dh_next, cache_dict[t-1], Wh)\n",
    "        dh_next = dh_prev\n",
    "        dWh += dWh_step\n",
    "        dWe += dWe_step\n",
    "        db1 += db1_step\n",
    "    return dWh, dWe, db1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atop each RNN Unit, sits an Affine layer which takes the vector $h^{(t)}$ as input, applies an Affine transformation, and computes the Softmax Probability. The parameters of this layer are $U \\space (Dim: D_{h} \\times V )$ and $b_2 \\space (Dim: V \\times 1)$. \n",
    "\n",
    "We do not have to implement a separate Affine layer for each time-step. Unlike in the case of an RNN Unit where the computation inside it depended on the output of its previous unit, the Affine computations at each time-step are independent of each other. Therefore, once we have computed $h^{(t)}$ for each time-step, we will perform the Affine computation for ALL $T$ time-steps in one go, taking into impact the contribution from ALL mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(h, U, b2):\n",
    "    N, T, Dh = h.shape\n",
    "    V = b2.shape[0]\n",
    "    theta = (np.matmul(h.reshape(N*T, Dh), U.T) + b2).reshape(N, T, V)\n",
    "    cache = U, b2, h\n",
    "    return theta, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_backward(dtheta, cache):\n",
    "    U, b2, h = cache\n",
    "    Dh = U.shape[1]\n",
    "    N, T, V = dtheta.shape\n",
    "    dh = np.matmul(dtheta.reshape(N*T, V), U).reshape(N, T, Dh)\n",
    "    dU = np.matmul((dtheta.reshape(N*T, V).T), h.reshape(N*T, Dh))\n",
    "    db2 = dtheta.sum(axis=(0,1))\n",
    "    return dh, dU, db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Evaluation of Gradients to Check Correctness of our Implementation\n",
    "If you are familiar with how to numerically check gradients for a network, you can skip this section, and move on to [New Section](#sec_id)\n",
    "\n",
    "Below, the function $eval\\_grad$ evaluates the gradient of a given function $f$ at a point $x$. This point $x$ can be multidimensional, for example I will use the $2D$ matrix $W_h$ as a 'point'. The 'gradient' is basically the change in Loss due to an infinitesimally small perturbation to the point $x$.\n",
    "\n",
    "Notice in the code below that I have multiplied by $dh$ to calculate the gradient. This is because we are going to be passing $rnn\\_forward$ and $rnn\\_step\\_forward$ for the argument $f$. Both these functions return the vector $h$ and not the scalar Loss which we need to compute the gradient w.r.t point $x$. Therefore we need to multiple by $dh$ to get our gradient, which is what we pass for the argument $df$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_grad(f, x, df):\n",
    "    grad = np.zeros_like(x)\n",
    "    epsilon = 1e-5\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        orig_val = x[idx]\n",
    "        x[idx] = orig_val + epsilon\n",
    "        fwd_fx = f(x)\n",
    "        x[idx] = orig_val - epsilon\n",
    "        bck_fx = f(x)\n",
    "        grad[idx] = np.sum((fwd_fx - bck_fx)*df/(epsilon*2))\n",
    "        x[idx] = orig_val\n",
    "        it.iternext()\n",
    "    return grad\n",
    "def rel_error(x, y):\n",
    "    # Returns relative error\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in dWh: 3.699756155582621e-11\n",
      "Error in dWe: 2.8456523039254035e-10\n",
      "Error in db1: 2.1963488162818666e-11\n",
      "Error in dh_prev: 2.3951187440793865e-11\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Check Gradients for single RNN Unit \"\"\"\n",
    "\n",
    "np.random.seed(10151)\n",
    "N, Dh, d, V = 2, 5, 3, 50\n",
    "# Parameters\n",
    "Wh = np.random.randn(Dh, Dh)\n",
    "b1 = np.random.randn(Dh)\n",
    "We = np.random.randn(Dh, d)\n",
    "\n",
    "# Inputs\n",
    "x_t = np.random.randn(N, d)\n",
    "h_prev = np.random.randn(N, Dh)\n",
    "\n",
    "# Test functions\n",
    "fWh = lambda Wh: rnn_step_forward(x_t, h_prev, Wh, We, b1)[0]\n",
    "fWe = lambda We: rnn_step_forward(x_t, h_prev, Wh, We, b1)[0]\n",
    "fb1 = lambda b1: rnn_step_forward(x_t, h_prev, Wh, We, b1)[0]\n",
    "fh_prev = lambda h_prev: rnn_step_forward(x_t, h_prev, Wh, We, b1)[0]\n",
    "\n",
    "# Evaluate test functions\n",
    "h_next, cache_step = rnn_step_forward(x_t, h_prev, Wh, We, b1)\n",
    "dh_next = np.random.randn(*h_next.shape)\n",
    "dWh, dWe, db1, dh_prev = rnn_step_backward(dh_next, cache_step, Wh)\n",
    "dWh_num = eval_grad(fWh, Wh, dh_next)\n",
    "dWe_num = eval_grad(fWe, We, dh_next)\n",
    "db1_num = eval_grad(fb1, b1, dh_next)\n",
    "dh_prev_num = eval_grad(fh_prev, h_prev, dh_next)\n",
    "print('Error in dWh: {}'.format(rel_error(dWh_num, dWh)))\n",
    "print('Error in dWe: {}'.format(rel_error(dWe_num, dWe)))\n",
    "print('Error in db1: {}'.format(rel_error(db1_num, db1)))\n",
    "print('Error in dh_prev: {}'.format(rel_error(dh_prev_num, dh_prev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in dWh: 9.831792282221475e-10\n",
      "Error in dWe: 1.0017182281175608e-10\n",
      "Error in db1: 9.769693584166271e-11\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Check Gradients for the entire RNN \"\"\"\n",
    "\n",
    "np.random.seed(10151)\n",
    "T, N, Dh, d, V = 10, 2, 5, 3, 50\n",
    "# Parameters\n",
    "Wh = np.random.randn(Dh, Dh)\n",
    "b1 = np.random.randn(Dh)\n",
    "We = np.random.randn(Dh, d)\n",
    "\n",
    "# Inputs\n",
    "x = np.random.randn(N, T, d)\n",
    "h_0 = np.random.randn(N, Dh)\n",
    "\n",
    "# Test functions\n",
    "fWh = lambda Wh: rnn_forward(T, x, h_0, Wh, We, b1)[0]\n",
    "fWe = lambda We: rnn_forward(T, x, h_0, Wh, We, b1)[0]\n",
    "fb1 = lambda b1: rnn_forward(T, x, h_0, Wh, We, b1)[0]\n",
    "\n",
    "# Evaluate test functions\n",
    "h, cache_dict = rnn_forward(T, x, h_0, Wh, We, b1)\n",
    "dh = np.random.randn(*h.shape)\n",
    "dWh, dWe, db1 = rnn_backward(dh, cache_dict, Wh, We, b1)\n",
    "dWh_num = eval_grad(fWh, Wh, dh)\n",
    "dWe_num = eval_grad(fWe, We, dh)\n",
    "db1_num = eval_grad(fb1, b1, dh)\n",
    "print('Error in dWh: {}'.format(rel_error(dWh_num, dWh)))\n",
    "print('Error in dWe: {}'.format(rel_error(dWe_num, dWe)))\n",
    "print('Error in db1: {}'.format(rel_error(db1_num, db1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in dU: 7.956108235981939e-10\n",
      "Error in db2: 1.578675630521908e-09\n",
      "Error in dh: 1.0117441876769132e-09\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Check Gradients for the Affine layer \"\"\"\n",
    "\n",
    "np.random.seed(10151)\n",
    "N, T, Dh, V = 5, 10, 6, 50\n",
    "\n",
    "# Parameters\n",
    "U = np.random.randn(V, Dh)\n",
    "b2 = np.random.rand(V)\n",
    "\n",
    "# Inputs\n",
    "h = np.random.randn(N, T, Dh)\n",
    "\n",
    "# Test Functions\n",
    "fU = lambda U: affine_forward(h, U, b2)[0]\n",
    "fb2 = lambda b2: affine_forward(h, U, b2)[0]\n",
    "fh = lambda h: affine_forward(h, U, b2)[0]\n",
    "\n",
    "# Evaluate test functions\n",
    "theta, cache = affine_forward(h, U, b2)\n",
    "dtheta = np.random.randn(*theta.shape)\n",
    "dh, dU, db2 = affine_backward(dtheta, cache)\n",
    "\n",
    "dU_num = eval_grad(fU, U, dtheta)\n",
    "db2_num = eval_grad(fb2, b2, dtheta)\n",
    "dh_num = eval_grad(fh, h, dtheta)\n",
    "\n",
    "print('Error in dU: {}'.format(rel_error(dU_num, dU)))\n",
    "print('Error in db2: {}'.format(rel_error(db2_num, db2)))\n",
    "print('Error in dh: {}'.format(rel_error(dh_num, dh)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec_id'></a>\n",
    "### New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
