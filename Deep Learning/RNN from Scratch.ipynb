{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/RNN Diagram.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-np.clip(x, -500, 500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_forward(e_t, h_prev, Wh, We, b1):\n",
    "    # Dimensions of Variables:\n",
    "        # h_prev ~ N x Dh (N is size of minibatch)\n",
    "        # Wh ~ Dh x Dh\n",
    "        # Wx ~ De x Dh\n",
    "        # b ~ Dh\n",
    "    h_next = sigmoid(np.matmul(h_prev, Wh.T) + np.matmul(e_t, We.T) + b1)\n",
    "    cache = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(e, h_0, Wh, We, b1):\n",
    "    N, T, D = e.shape\n",
    "    _, Dh = h0.shape\n",
    "    h = np.zeros((N, T, D))\n",
    "    h_prev = h_0\n",
    "    cache = {}\n",
    "    for t in range(T):\n",
    "        h[:, t, :], cache_t = rnn_step_forward(e[:,t,:], h_prev, Wh, We, b1) \n",
    "        h_prev = h[:, t, :]\n",
    "        cache.update({t : cache_t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_backward(dh_next, cache):\n",
    "    h_prev, h_next = cache\n",
    "    dsigmoid = h_next*(1 - h_next)\n",
    "    dWh = np.matmul(dh_next*dsigmoid, h_prev.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atop each RNN Unit, sits an Affine layer which takes the vector $h^{(t)}$ as input, applies an Affine transformation, and computes the Softmax Probability. The parameters of this layer are $U \\space (Dim: D_{h} \\times \\lvert V \\rvert)$ and $b_2 \\space (Dim: \\lvert V \\rvert \\times 1)$. \n",
    "\n",
    "We do not have to implement a separate Affine layer for each time-step. Unlike in the case of an RNN Unit where the computation inside it depended on the output of its previous unit, the Affine computations at each time-step are independent of each other. Therefore, once we have computed $h^{(t)}$ for each time-step, we will perform the Affine computation for ALL time-steps in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(h, U, b2):\n",
    "    N, T, Dh = h.shape\n",
    "    V = b2.shape[0]\n",
    "    out = (np.matmul(h.reshape(N*T, Dh), U) + b2).reshape(N, T, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_backward():\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
